{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Week 7: Intro to NLP\n",
    "##### Part 1\n",
    "\n",
    "Exercises: NLPP Chapter 1\n",
    "1.3 Searching Text\n",
    "\n",
    "Concordance: shows us every occurrence of a given word, together with some context. \n",
    "\n",
    "Examples:\n",
    ">>> text2.concordance(\"affection\")\n",
    ", however , and , as a mark of his affection for the three girls , he left them\n",
    "t . It was very well known that no affection was ever supposed to exist between\n",
    "deration of politeness or maternal affection on the side of the former , the tw\n",
    "d the suspicion -- the hope of his affection for me may warrant , without impru\n",
    "hich forbade the indulgence of his affection . She knew that his mother neither\n",
    "rd she gave one with still greater affection . Though her late conversation wit\n",
    " can never hope to feel or inspire affection again , and if her home be uncomfo\n",
    "m of the sense , elegance , mutual affection , and domestic comfort of the fami\n",
    ", and which recommended him to her affection beyond every thing else . His soci\n",
    "ween the parties might forward the affection of Mr . Willoughby , an equally st\n",
    " the most pointed assurance of her affection . Elinor could not be surprised at\n",
    "he natural consequence of a strong affection in a young and ardent mind . This \n",
    " opinion . But by an appeal to her affection for her mother , by representing t\n",
    " every alteration of a place which affection had established as perfect with hi\n",
    "e will always have one claim of my affection , which no other can possibly shar\n",
    "f the evening declared at once his affection and happiness . \" Shall we see you\n",
    "ause he took leave of us with less affection than his usual behaviour has shewn\n",
    "ness .\" \" I want no proof of their affection ,\" said Elinor ; \" but of their en\n",
    "onths , without telling her of his affection ;-- that they should part without \n",
    "ould be the natural result of your affection for her . She used to be all unres\n",
    "distinguished Elinor by no mark of affection . Marianne saw and listened with i\n",
    "th no inclination for expense , no affection for strangers , no profession , an\n",
    "till distinguished her by the same affection which once she had felt no doubt o\n",
    "al of her confidence in Edward ' s affection , to the remembrance of every mark\n",
    " was made ? Had he never owned his affection to yourself ?\" \" Oh , no ; but if \n",
    "\n",
    " >>> text7.concordance(\"technology\")\n",
    "Displaying 11 of 11 matches:\n",
    " Cray-3 's tricky , unproven chip technology . The SEC documents describe thos\n",
    "t , currently chairman of Seagate Technology , led the team that *T*-32 develo\n",
    " Week '' at the Mara Institute of Technology near Kuala Lumpur and urged other\n",
    "ftware , said 0 it introduced new technology in mechanical design automation t\n",
    "es access to American markets and technology . But for small American companie\n",
    "prove Japanese access to American technology and market knowledge , they feed \n",
    "s some drawbacks . The additional technology , personnel training and promotio\n",
    "om the Massachusetts Institute of Technology . Mr. Hahn agrees that he has a `\n",
    "the market . You ca n't hold back technology . '' And the practice should n't \n",
    "ry has *T*-2 0 * to debate risk , technology and innovation *T*-3 *T*-1 . Too \n",
    "growth factor via recombinant DNA technology . Sandoz Ltd. has licensed certai\n",
    "\n",
    ">>> text4.concordance(\"existence\")\n",
    "Displaying 25 of 34 matches:\n",
    " whatever form it may appear . The existence of such a government as ours for a\n",
    "le them to maintain their place in existence and to prepare them in time for th\n",
    "vement permitted only a transitory existence . In this great nation there is bu\n",
    "ance with an usage coeval with the existence of our Federal Constitution , and \n",
    "d me , the Revolution that gave us existence as one people was achieved at the \n",
    "tinct sovereignties were in actual existence , whose cordial union was essentia\n",
    " was to bind it and perpetuate its existence was the affectionate attachment be\n",
    "in of all the republics with whose existence and fall their writings have made \n",
    "ss of the people to believe in its existence or from the influence of designing\n",
    "elieve that at every period of our existence as a nation there has existed , an\n",
    "the earlier stages of our national existence the opinion prevailed with some th\n",
    "itute the strength of our national existence . In reference to the Army and Nav\n",
    "ned to the nations which gave them existence , and within their legitimate juri\n",
    "even seriously endangered the very existence of the Union . Nor has the danger \n",
    "the ills you fly from have no real existence ? Will you , while the certain ill\n",
    "s at the beginning of our national existence . The effects of the late civil st\n",
    "in a struggle threatening the very existence of the nation . I performed a cons\n",
    " for half a century threatened the existence of the Union , was closed at last \n",
    "e . But the responsibility for the existence of slavery did not rest upon the S\n",
    "to its second century of organized existence under the Constitution and that we\n",
    "nce , to also briefly refer to the existence of certain conditions and tendenci\n",
    "ication of political methods . The existence of immense aggregations of kindred\n",
    "the enforcement of the laws now in existence and the recommendation and support\n",
    " not been obliged to fight for our existence against any alien race ; and yet o\n",
    "e now face other perils , the very existence of which it was impossible that th\n",
    "\n",
    ">>> text1.similar(\"monstrous\")\n",
    "true contemptible christian abundant few part mean careful puzzled\n",
    "mystifying passing curious loving wise doleful gamesome singular\n",
    "delightfully perilous fearless\n",
    ">>> text2.similar(\"monstrous\")\n",
    "very so exceedingly heartily a as good great extremely remarkably\n",
    "sweet vast amazingly\n",
    "\n",
    ">>> text2.similar(\"love\")\n",
    "affection sister heart mother time see town life it dear elinor\n",
    "marianne me word family her him do regard head\n",
    ">>> text4.similar(\"love\")\n",
    "people freedom time hope confidence government union peace strength\n",
    "purpose country nation future world way security congress spirit\n",
    "desire safety\n",
    "\n",
    ">>> text1.common_contexts([\"Whales\", \"more\"])\n",
    "the_and the_to\n",
    "\n",
    "\n",
    "*** make this :\n",
    "text4.dispersion_plot([\"citizens\", \"democracy\", \"freedom\", \"duties\", \"America\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 1.4\n",
    "Text5.count(\"lol) = 704 times\n",
    "As a % = 1.5640968673628082 \n",
    "*** review this number\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 2: Texts as Lists of Words\n",
    "\n",
    "2.1 Lists\n",
    "\n",
    ">>> ex1 = [\"Hello\", \"my\", \"name\", \"is\"]\n",
    ">>> ex1\n",
    "['Hello', 'my', 'name', 'is']\n",
    ">>> ex2 = [\"Marlo\", \"Anzarut\"]\n",
    ">>> ex1 + ex2\n",
    "['Hello', 'my', 'name', 'is', 'Marlo', 'Anzarut']\n",
    ">>> len(ex1 + ex2)\n",
    "6\n",
    "\n",
    ">>> sentence1 = [\"DTU\", \"is\", \"awesome\", \"!\"]\n",
    ">>> sentence1[0]\n",
    "'DTU'\n",
    ">>> len(sentence1)\n",
    "4\n",
    ">>> sentence1[0]\n",
    "'DTU'\n",
    ">>> sentence1[0] = \"Marlo\"\n",
    ">>> sentence1\n",
    "['Marlo', 'is', 'awesome', '!']\n",
    ">>> sentence1[0] = \"Elinor & Leland\"\n",
    ">>> sentence1\n",
    "['Elinor & Leland', 'is', 'awesome', '!']\n",
    ">>> sentence1[1] = \"are\"\n",
    ">>> sentence1\n",
    "['Elinor & Leland', 'are', 'awesome', '!']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PART 3: Simple stats\n",
    ">>> fdist2 = FreqDist(text2)\n",
    ">>> print(fdist2)\n",
    "<FreqDist with 6833 samples and 141576 outcomes>\n",
    ">>> fdist2.most_common(50)\n",
    "[(',', 9397), ('to', 4063), ('.', 3975), ('the', 3861), ('of', 3565), ('and', 3350), ('her', 2436), ('a', 2043), ('I', 2004), ('in', 1904), ('was', 1846), ('it', 1568), ('\"', 1506), (';', 1419), ('she', 1333), ('be', 1305), ('that', 1297), ('for', 1234), ('not', 1212), ('as', 1179), ('you', 1037), ('with', 971), ('had', 969), ('his', 941), ('he', 895), (\"'\", 883), ('have', 807), ('at', 806), ('by', 737), ('is', 728), ('.\"', 721), ('s', 700), ('Elinor', 684), ('on', 676), ('all', 642), ('him', 633), ('so', 617), ('but', 597), ('which', 592), ('could', 568), ('Marianne', 566), ('my', 551), ('Mrs', 530), ('from', 527), ('would', 507), ('very', 492), ('no', 488), ('their', 463), ('them', 462), ('--', 461)]\n",
    ">>> fdist2['home']\n",
    "69\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 3.2\n",
    "\n",
    ">>> vocab = set(text1)\n",
    ">>> long_words = [w for w in vocab if len(w) > 15]\n",
    ">>> sorted(long_words)\n",
    "['CIRCUMNAVIGATION', 'Physiognomically', 'apprehensiveness', 'cannibalistically', 'characteristically', 'circumnavigating', 'circumnavigation', 'circumnavigations', 'comprehensiveness', 'hermaphroditical', 'indiscriminately', 'indispensableness', 'irresistibleness', 'physiognomically', 'preternaturalness', 'responsibilities', 'simultaneousness', 'subterraneousness', 'supernaturalness', 'superstitiousness', 'uncomfortableness', 'uncompromisedness', 'undiscriminating', 'uninterpenetratingly']\n",
    ">>> long_words = [w for w in vocab if len(w) > 20]\n",
    ">>> sorted(long_words)\n",
    "[]\n",
    "\n",
    "If you change the variable names, you will get the same results, as long as you change both variable names to match. For instance, if you write vocab instead of V, in both lines of code, you will get the same answer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 4: Making Decisions and Taking Control\n",
    "\n",
    ">>> sorted(w for w in set(text1) if w.endswith('ableness'))\n",
    "['comfortableness', 'honourableness', 'immutableness', 'indispensableness', 'indomitableness', 'intolerableness', 'palpableness', 'reasonableness', 'uncomfortableness']\n",
    "-- In this example, we are looping through words in text 1 (Moby Dick) which are sorted, and compiling a list of words that end with 'ableness'.\n",
    "\n",
    ">>> sorted(term for term in set(text4) if 'gnt' in term)\n",
    "['Sovereignty', 'sovereignties', 'sovereignty']\n",
    "-- In this example, we are looping throguh the sorted terms of text4, and compiling a list of words that contain the term 'gnt'.'gnt' is not a very common combination of letters, which explains why we found only 3 words, all consisting of the core word 'sovereign'.\n",
    "\n",
    ">>> sorted(item for item in set(text6) if item.istitle())\n",
    "['A', 'Aaaaaaaaah', 'Aaaaaaaah', 'Aaaaaah', 'Aaaah', 'Aaaaugh', 'Aaagh', ...]\n",
    "-- In this example, we are looping throguh a sorted set of items, and trying to find words that begin with a capital letter, by using istitle(). This explains why our list is incredibly long.\n",
    "\n",
    ">>> sorted(item for item in set(sent7) if item.isdigit())\n",
    "['29', '61']\n",
    "-- In this example, we are looping through a sorted set of items, and compiling a list of items that are comprised of all digits.\n",
    "\n",
    "##### HOMEWORK EXAMPLES:\n",
    "\n",
    ">>> sorted(w for w in set(text7) if '-' in w and 'index' in w)\n",
    "['Stock-index', 'index-arbitrage', 'index-fund', 'index-options', 'index-related', 'stock-index']\n",
    "-- In this example, we are looping throguh a sorted set of words of text7, and compiling a list of words that have a '-' in the middle of the word.\n",
    "\n",
    ">>> sorted(wd for wd in set(text3) if wd.istitle() and len(wd) > 10)\n",
    "['Abelmizraim', 'Allonbachuth', 'Beerlahairoi', 'Canaanitish', 'Chedorlaomer', 'Girgashites', 'Hazarmaveth', 'Hazezontamar', 'Ishmeelites', 'Jegarsahadutha', 'Jehovahjireh', 'Kirjatharba', 'Melchizedek', 'Mesopotamia', 'Peradventure', 'Philistines', 'Zaphnathpaaneah']\n",
    "-- In this example, we are looping throguh the sorted set of words of text3, and are compiling a list of words that have an initial capitalized letter and are of length greater than 10.\n",
    "\n",
    ">>> sorted(w for w in set(sent7) if not w.islower())\n",
    "[',', '.', '29', '61', 'Nov.', 'Pierre', 'Vinken']\n",
    "-- In this example, we are looping through the sorted words of the set of sent7, and we are compiling a list of words that are not lower case bu negating the '.islower()' function.\n",
    "\n",
    ">>> sorted(t for t in set(text2) if 'cie' in t or 'cei' in t)\n",
    "['ancient', 'ceiling', 'conceit', 'conceited', 'conceive', 'conscience', 'conscientious', 'conscientiously', 'deceitful', 'deceive', 'deceived', 'deceiving', 'deficiencies', 'deficiency', 'deficient', 'delicacies', 'excellencies', 'fancied', 'insufficiency', 'insufficient', 'legacies', 'perceive', 'perceived', 'perceiving', 'prescience', 'prophecies', 'receipt', 'receive', 'received', 'receiving', 'society', 'species', 'sufficient', 'sufficiently', 'undeceive', 'undeceiving']\n",
    "-- In this example, we are looping through the sorted words of the set of text2, and compiling a list of words that contain 'cie' or 'cei' in it.\n",
    "\n",
    "My examples:\n",
    ">>> sorted(w for w in set(text2) if not w.islower() and len(w)  >10)\n",
    "['Astonishment', 'Beautifully', 'Comparisons', 'Considering', 'Conversation', 'Disappointed', 'Disappointment', 'Dorsetshire', 'Extravagance', 'Fortunately', 'INconvenience', 'Marlborough', 'Opportunity', 'Preparation', 'Prescriptions', 'Recollecting', 'Richardsons', 'Sensibility', 'Shakespeare', 'Somersetshire', 'Thunderbolts', 'Unaccountable', 'Undoubtedly', 'Westminster', 'Willoughbys']\n",
    "-- In this example, we sorted the words of text2, then looped through this set to find words that have a capitalized letter and of length longer than 10 characters.\n",
    "\n",
    ">>> sorted(t for t in set(text5) if 'and' in t)\n",
    "['#prideIsland', '#prideisland', 'Cleveland', 'Hand', 'Holland', 'Randy', 'and', 'backfrontsidewaysandallaroundtheworld', 'band', 'bandito', 'bandsaw', 'candy', 'chatland', 'commanded', 'demand', 'e-husband', 'england', 'hand', 'handheld', 'handing', 'handle', 'hands', 'handsome', 'handy', 'handyMan', 'husband', 'island', 'land', 'mainland', 'maryland', 'neverneverland', 'pandas', 'peace-and-quiet', 'poooland', 'randy', 'sand', 'stand', 'standin', 'standing', 'tonawanda', 'understand', 'wanders']\n",
    "-- In this example, we looped through the sorted words of text5, and compiled a list of words that contain 'and' in it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 5: Automatic Natural Language Understanding\n",
    "\n",
    "Part 5.4 : complete this section!\n",
    "Part 5.5 : also\n",
    "\n",
    "##### Section 8: extra exercises at the end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**** Make sure to do these tasks:\n",
    "- Create your own version of a dispersion plot (\"your own version\" means another text and different word).\n",
    "\n",
    "- Explain in your own words what aspect of language lexical diversity describes.\n",
    "Lexical diversity describes the complexity of a text. In other words, it explains the difficult to read a text. For example, if you have two texts, one includes many repetitive words and the other uses synonyms instead of repeating those words. The second one is more difficult to read and has more 'lexical diversity' than the first text. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Putting things into practice with the abstract dataset\n",
    "\n",
    "##### Prelude to Exercise 1: Some theory on the Zipf's law.\n",
    "Exercise 1: Tokenization and Zipf's Law."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import random\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# NEED TO LOAD abstract dataframe from week 2, abstract_df\n",
    "\n",
    "def inverted_to_plaintext(inverted_index):\n",
    "    text = ' '.join(inverted_index.keys())\n",
    "    return text\n",
    "\n",
    "abstract_df['text'] = abstract_df['inverted_index'].apply(inverted_to_plaintext)\n",
    "\n",
    "def tokenize_text(text):\n",
    "    text = re.sub(r'http\\S+', '', text)  # Remove URL\n",
    "    text = re.sub(r'[^a-zA-Z]', '', text) # Remove punctuation, symbols, numbers\n",
    "    text = text.lower()\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "abstract_df['tokens'] = abstract_df['text'].apply(tokenize_text)\n",
    "\n",
    "tokens_list = []\n",
    "for tokens in abstract_df['tokens']:\n",
    "    tokens_list.extend(tokens)\n",
    "\n",
    "word_frequency = Counter(tokens_list)\n",
    "most_common_words = word_frequency.most_common(10)\n",
    "print(\"The 10 most common words are:\", most_common_words)\n",
    "\n",
    "def freq_rank_plot(word_freqency):\n",
    "    words, frequencies = zip(*word_frequency)\n",
    "    ranks = np.arrange(1, len(words)+ 1)\n",
    "    plt.loglog(ranks, frequencies, marker='.')\n",
    "    plt.title(\"Word Frequency Rank Plot (Zipf's Law)\")\n",
    "    plt.xlabel('Rank')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()\n",
    "\n",
    "freq_rank_plot(most_common_words)\n",
    "\n",
    "random_text = ''.join(random.choices(\"abcdefg \", k=10000))\n",
    "\n",
    "random_tokens = tokenize_text(random_text)\n",
    "\n",
    "random_word_freq = nltk.FreqDist(random_tokens)\n",
    "\n",
    "freq_rank_plot(random_word_freq)\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "filtered_tokens = [token for token in tokens_list if token not in stop_words]\n",
    "\n",
    "filtered_word_freq = nltk.FreqDist(filtered_tokens)\n",
    "\n",
    "freq_rank_plot(filtered_word_freq)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.6 Comparison with Random Words\n",
    "**** Share your insights on Zipf's Law based on this comparison.\n",
    "\n",
    "1.7 Excluding Stopwards\n",
    "*** Repeat Steps 2 to 5 with the refined tokens list.\n",
    "*** Observe and describe any changes in your findings.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
